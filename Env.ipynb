{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt(filename, data):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：将字符串 \"Hello, World!\" 追加到文件 \"example.txt\" 中\n",
    "save_txt(\"example.txt\", \"Hello, World!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "def test_save_txt():\n",
    "    # 创建一个临时文件\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    temp_file_path = temp_file.name\n",
    "    temp_file.close()\n",
    "\n",
    "    # 定义要写入文件的数据\n",
    "    data_to_write = \"This is a test.\\n\"\n",
    "\n",
    "    # 调用 save_txt 函数\n",
    "    save_txt(temp_file_path, data_to_write)\n",
    "\n",
    "    # 读取文件内容以验证数据\n",
    "    with open(temp_file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # 断言文件内容与预期相符\n",
    "    assert file_content == data_to_write, f\"Expected '{data_to_write}', but got '{file_content}'\"\n",
    "\n",
    "    # 清理临时文件\n",
    "    os.remove(temp_file_path)\n",
    "\n",
    "# 运行测试用例\n",
    "test_save_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv(li):\n",
    "    result = ''\n",
    "    for i in li:\n",
    "        result += '{}, '.format(i)\n",
    "    return result.rstrip(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_list_to_csv():\n",
    "    # 定义一个测试列表\n",
    "    test_list = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # 调用 list_to_csv 函数，将测试列表转换为 CSV 格式的字符串\n",
    "    csv_string = list_to_csv(test_list)\n",
    "    \n",
    "    # 验证结果字符串是否与预期的 CSV 格式字符串相匹配\n",
    "    expected_csv_string = \"1, 2, 3, 4, 5\"\n",
    "    assert csv_string == expected_csv_string, f\"Expected '{expected_csv_string}', but got '{csv_string}'\"\n",
    "    \n",
    "    # 测试空列表的情况\n",
    "    empty_list = []\n",
    "    csv_string_empty = list_to_csv(empty_list)\n",
    "    assert csv_string_empty == \"\", f\"Expected empty string for empty list, but got '{csv_string_empty}'\"\n",
    "\n",
    "# 调用测试函数\n",
    "test_list_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regeneralize(result, mean, std):  # 输入的result是个array\n",
    "    mean_matrix = mean\n",
    "    std_matrix = std\n",
    "    result_real_scale = (result * std_matrix) + mean_matrix\n",
    "    return result_real_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: regeneralized result matches expected result.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_regeneralize():\n",
    "    # 假设我们有一个已经标准化的数据集\n",
    "    result = np.array([-1.0, 0.0, 1.0]) # 标准化后的数据\n",
    "    mean = np.array([0.0]) # 原始数据的均值\n",
    "    std = np.array([1.0]) # 原始数据的标准差\n",
    "\n",
    "    # 调用regeneralize函数\n",
    "    regeneralized_result = regeneralize(result, mean, std)\n",
    "\n",
    "    # 验证结果\n",
    "    expected_result = np.array([-1.0, 0.0, 1.0]) # 预期的结果\n",
    "    assert np.allclose(regeneralized_result, expected_result), \"Test failed: regeneralized result does not match expected result.\"\n",
    "    print(\"Test passed: regeneralized result matches expected result.\")\n",
    "\n",
    "# 运行测试\n",
    "test_regeneralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(prefix, folder):\n",
    "    result = []\n",
    "    file_list = os.listdir(folder)\n",
    "    for file in file_list:\n",
    "        if file.endswith(prefix):\n",
    "            result.append(os.path.join(folder, file))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def test_get_file_list():\n",
    "    # 定义测试文件夹和后缀\n",
    "    folder = './data/' # 请确保这是实际的文件夹路径\n",
    "    prefix = '.csv'\n",
    "    \n",
    "    # 调用函数获取文件列表\n",
    "    file_list = get_file_list(prefix, folder)\n",
    "    \n",
    "    # 预期结果：文件夹中所有以 .csv 结尾的文件\n",
    "    expected_files = [\n",
    "        os.path.join(folder, 'vertical_all_A1.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A2.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A3.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A4.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A5.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A6.csv')\n",
    "    ]\n",
    "    \n",
    "    # 断言函数返回的文件列表与预期结果相同\n",
    "    assert file_list == expected_files, f\"Expected {expected_files}, but got {file_list}\"\n",
    "\n",
    "# 运行测试\n",
    "test_get_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是的，这两个功能在某种程度上是相似的，但它们的实现方式和目的有所不同。\n",
    "\n",
    "1. **使用 `file.endswith(prefix)` 的功能**：\n",
    "   - 这个功能的目的是找到所有以特定后缀结尾的文件。\n",
    "   - 它使用 `endswith` 方法来检查文件名是否以指定的后缀结尾。\n",
    "   - 这个方法适用于查找文件类型，例如所有的 `.txt` 文件。\n",
    "\n",
    "2. **使用 `re.match(f'^{prefix}', file)` 的功能**：\n",
    "   - 这个功能的目的是找到所有以特定前缀开头的文件。\n",
    "   - 它使用正则表达式来检查文件名是否以指定的前缀开头。\n",
    "   - 这个方法适用于查找以特定字符串开头的文件，例如所有以 `vertical_all_A` 开头的文件。\n",
    "\n",
    "虽然这两个功能在某种程度上是相似的，但它们的使用场景和实现方式有所不同。`endswith` 方法是一个简单的字符串方法，用于检查字符串是否以指定的后缀结尾。而 `re.match` 是一个正则表达式函数，提供了更强大的模式匹配功能，可以用于检查字符串是否以指定的前缀开头。\n",
    "\n",
    "在你的情况下，由于你需要查找以特定前缀开头的文件，使用正则表达式是更合适的选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def get_file_list(prefix, folder):\n",
    "    result = []\n",
    "    file_list = os.listdir(folder)\n",
    "    for file in file_list:\n",
    "        if re.match(f'^{prefix}', file):\n",
    "            result.append(os.path.join(folder, file))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vertical_all_A1.csv', 'vertical_all_A2.csv', 'vertical_all_A3.csv', 'vertical_all_A4.csv', 'vertical_all_A5.csv', 'vertical_all_A6.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def test_get_file_list():\n",
    "    # 定义测试文件夹和前缀\n",
    "    folder = './data/' # 请替换为实际的文件夹路径\n",
    "    prefix = 'vertical_all_A'\n",
    "    print(os.listdir(folder))\n",
    "    # 调用函数获取文件列表\n",
    "    file_list = get_file_list(prefix, folder)\n",
    "    \n",
    "    # 预期结果：文件夹中所有以 'vertical_all_A' 开头的 .csv 文件\n",
    "    expected_files = [\n",
    "        os.path.join(folder, 'vertical_all_A1.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A2.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A3.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A4.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A5.csv'),\n",
    "        os.path.join(folder, 'vertical_all_A6.csv')\n",
    "    ]\n",
    "    \n",
    "    # 断言函数返回的文件列表与预期结果相同\n",
    "    assert file_list == expected_files, f\"Expected {expected_files}, but got {file_list}\"\n",
    "\n",
    "# 运行测试\n",
    "test_get_file_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_var(v, filename):\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(v, f)\n",
    "    f.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: The dictionary was saved and loaded correctly.\n"
     ]
    }
   ],
   "source": [
    "def test_save_var():\n",
    "    # 创建一个简单的字典对象\n",
    "    test_dict = {'key': 'value', 'number': 42}\n",
    "    \n",
    "    # 使用 save_var 函数保存字典到文件\n",
    "    filename = 'test_dict.pkl'\n",
    "    save_var(test_dict, filename)\n",
    "    \n",
    "    # 读取文件并验证内容\n",
    "    with open(filename, 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    \n",
    "    # 验证加载的字典与原始字典相同\n",
    "    assert loaded_dict == test_dict, \"Loaded dictionary does not match the original one.\"\n",
    "    \n",
    "    print(\"Test passed: The dictionary was saved and loaded correctly.\")\n",
    "\n",
    "# 运行测试\n",
    "test_save_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(data, size):\n",
    "    i = 0\n",
    "    while i+size < data.shape[1]:\n",
    "        yield data[:, i:i+size, :].mean(axis=1)\n",
    "        i += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2.,  3.],\n",
      "       [22., 23.],\n",
      "       [42., 43.]]), array([[ 6.,  7.],\n",
      "       [26., 27.],\n",
      "       [46., 47.]]), array([[10., 11.],\n",
      "       [30., 31.],\n",
      "       [50., 51.]]), array([[14., 15.],\n",
      "       [34., 35.],\n",
      "       [54., 55.]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 创建一个假的数据集，形状为(3, 10, 2)\n",
    "data = np.array([\n",
    "    [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]],\n",
    "    [[21, 22], [23, 24], [25, 26], [27, 28], [29, 30], [31, 32], [33, 34], [35, 36], [37, 38], [39, 40]],\n",
    "    [[41, 42], [43, 44], [45, 46], [47, 48], [49, 50], [51, 52], [53, 54], [55, 56], [57, 58], [59, 60]]\n",
    "])\n",
    "\n",
    "# 使用shrink函数处理数据，块大小为2\n",
    "result = list(shrink(data, 2))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.,  3.],\n",
       "        [22., 23.],\n",
       "        [42., 43.]]),\n",
       " array([[ 6.,  7.],\n",
       "        [26., 27.],\n",
       "        [46., 47.]]),\n",
       " array([[10., 11.],\n",
       "        [30., 31.],\n",
       "        [50., 51.]]),\n",
       " array([[14., 15.],\n",
       "        [34., 35.],\n",
       "        [54., 55.]])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.mean = []\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = []\n",
    "        self.mean = []\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.data.append(val)\n",
    "        if type(val) == list:\n",
    "            self.mean.append(np.mean(val))\n",
    "        else:\n",
    "            self.mean.append(val)\n",
    "        self.count += n\n",
    "    \n",
    "    def get_latest(self, mean=True):\n",
    "        if mean:\n",
    "            return self.mean[-1]\n",
    "        else:\n",
    "            return self.data[-1]\n",
    "\n",
    "    def delta(self):\n",
    "        return abs(self.mean[-1] - self.mean[-2])\n",
    "\n",
    "    def bigger(self):\n",
    "        return self.mean[-1] - self.mean[-2] > 0\n",
    "\n",
    "    def check(self, val):\n",
    "        return (self.mean[-1] - np.mean(val)) < 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最新的平均值: 30.0\n",
      "最新的数据: [20, 30, 40]\n",
      "最新的平均值与前一个平均值之间的差值: 20.0\n",
      "最新的平均值是否大于前一个平均值: True\n",
      "传入的值与最新的平均值之间的关系: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个Record实例\n",
    "record = Record()\n",
    "\n",
    "# 更新实例，添加一个数值\n",
    "record.update(10)\n",
    "\n",
    "# 更新实例，添加一个列表\n",
    "record.update([20, 30, 40])\n",
    "\n",
    "# 获取最新的平均值\n",
    "latest_mean = record.get_latest(mean=True)\n",
    "print(f\"最新的平均值: {latest_mean}\")\n",
    "\n",
    "# 获取最新的数据\n",
    "latest_data = record.get_latest(mean=False)\n",
    "print(f\"最新的数据: {latest_data}\")\n",
    "\n",
    "# 计算最新的平均值与前一个平均值之间的差值\n",
    "delta = record.delta()\n",
    "print(f\"最新的平均值与前一个平均值之间的差值: {delta}\")\n",
    "\n",
    "# 检查最新的平均值是否大于前一个平均值\n",
    "is_bigger = record.bigger()\n",
    "print(f\"最新的平均值是否大于前一个平均值: {is_bigger}\")\n",
    "\n",
    "# 检查传入的值与最新的平均值之间的关系\n",
    "check_result = record.check([50, 60])\n",
    "print(f\"传入的值与最新的平均值之间的关系: {check_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Params():\n",
    "    \"\"\"Class that loads hyperparameters from a json file.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    params = Params(json_path)\n",
    "    print(params.learning_rate)\n",
    "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "    def save(self, json_path):\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "            \n",
    "    def update(self, json_path):\n",
    "        \"\"\"Loads parameters from json file\"\"\"\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']\"\"\"\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.01\n",
      "Updated learning rate: 0.02\n",
      "Updated batch size: 64\n",
      "Updated epochs: 200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 加载超参数\n",
    "params = Params('hyperparameters.json')\n",
    "print(\"Initial learning rate:\", params.learning_rate)\n",
    "\n",
    "# 修改超参数\n",
    "params.learning_rate = 0.02\n",
    "params.batch_size = 64\n",
    "params.epochs = 200\n",
    "\n",
    "# 保存修改后的超参数\n",
    "params.save('updated_hyperparameters.json')\n",
    "\n",
    "# 加载更新后的超参数\n",
    "updated_params = Params('updated_hyperparameters.json')\n",
    "print(\"Updated learning rate:\", updated_params.learning_rate)\n",
    "print(\"Updated batch size:\", updated_params.batch_size)\n",
    "print(\"Updated epochs:\", updated_params.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RunningAverage():\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.total/float(self.steps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "loss_avg = RunningAverage()\n",
    "loss_avg.update(2)\n",
    "loss_avg.update(4)\n",
    "print(loss_avg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def set_logger(log_path):\n",
    "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
    "\n",
    "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
    "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    logging.info(\"Starting training...\")\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        log_path: (string) where to log\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:This is a test log message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log content: \n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# 创建一个临时文件路径\n",
    "temp_log_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "\n",
    "# 调用 set_logger 函数\n",
    "set_logger(temp_log_file)\n",
    "\n",
    "# 记录一条日志消息\n",
    "logging.info(\"This is a test log message.\")\n",
    "\n",
    "# 检查日志文件\n",
    "with open(temp_log_file, 'r') as file:\n",
    "    log_content = file.read()\n",
    "    print(f\"Log content: {log_content}\")\n",
    "\n",
    "# 清理临时文件\n",
    "os.remove(temp_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"Saves dict of floats in json file\n",
    "\n",
    "    Args:\n",
    "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
    "        json_path: (string) path to json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试用例\n",
    "def test_save_dict_to_json():\n",
    "    # 创建一个字典\n",
    "    test_dict = {\n",
    "        \"key1\": 1,\n",
    "        \"key2\": 2.5,\n",
    "        \"key3\": 3.0,\n",
    "        \"key4\": 4.0\n",
    "    }\n",
    "    \n",
    "    # 指定保存的 JSON 文件路径\n",
    "    json_path = \"test_output.json\"\n",
    "    \n",
    "    # 调用函数保存字典到 JSON 文件\n",
    "    save_dict_to_json(test_dict, json_path)\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    assert os.path.exists(json_path), \"JSON file not created\"\n",
    "    \n",
    "    # 读取并检查 JSON 文件内容\n",
    "    with open(json_path, 'r') as f:\n",
    "        loaded_dict = json.load(f)\n",
    "        assert loaded_dict == test_dict, \"Loaded data does not match original data\"\n",
    "    \n",
    "    # 删除测试文件\n",
    "    os.remove(json_path)\n",
    "\n",
    "# 运行测试用例\n",
    "test_save_dict_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state, is_best, checkpoint):\n",
    "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
    "    checkpoint + 'best.pth.tar'\n",
    "\n",
    "    Args:\n",
    "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
    "        is_best: (bool) True if it is the best model seen till now\n",
    "        checkpoint: (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
    "        os.mkdir(checkpoint)\n",
    "    else:\n",
    "        print(\"Checkpoint Directory exists! \")\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory does not exist! Making directory test_checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "class TestSaveCheckpoint:\n",
    "    def setUp(self):\n",
    "        self.checkpoint_dir = \"test_checkpoint\"\n",
    "        # 创建一个简单的线性层模型\n",
    "        model = torch.nn.Linear(10, 1)\n",
    "        self.state = {\n",
    "            \"epoch\": 10,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": torch.optim.Adam(model.parameters()).state_dict()\n",
    "        }\n",
    "        self.is_best = True\n",
    "\n",
    "    def tearDown(self):\n",
    "        if os.path.exists(self.checkpoint_dir):\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "\n",
    "    def test_save_checkpoint(self):\n",
    "        save_checkpoint(self.state, self.is_best, self.checkpoint_dir)\n",
    "        assert os.path.exists(os.path.join(self.checkpoint_dir, 'last.pth.tar')), \"last.pth.tar not found\"\n",
    "        assert os.path.exists(os.path.join(self.checkpoint_dir, 'best.pth.tar')), \"best.pth.tar not found\"\n",
    "\n",
    "# 创建一个实例并运行测试\n",
    "test = TestSaveCheckpoint()\n",
    "test.setUp()\n",
    "test.test_save_checkpoint()\n",
    "test.tearDown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "class TestLoadCheckpoint:\n",
    "    def setUp(self):\n",
    "        self.checkpoint_dir = \"test_checkpoint\"\n",
    "        self.model = torch.nn.Linear(10, 1)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.state = {\n",
    "            \"epoch\": 10,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict()\n",
    "        }\n",
    "        self.is_best = True\n",
    "        save_checkpoint(self.state, self.is_best, self.checkpoint_dir)\n",
    "\n",
    "    def tearDown(self):\n",
    "        if os.path.exists(self.checkpoint_dir):\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "\n",
    "    def test_load_checkpoint(self):\n",
    "        # 加载检查点\n",
    "        loaded_state = load_checkpoint(os.path.join(self.checkpoint_dir, 'last.pth.tar'), self.model, self.optimizer)\n",
    "        # 检查模型和优化器的状态是否正确加载\n",
    "        assert self.state['epoch'] == loaded_state['epoch'], \"Epoch mismatch\"\n",
    "        assert self.state['state_dict'] == loaded_state['state_dict'], \"Model state_dict mismatch\"\n",
    "        assert self.state['optimizer_state_dict'] == loaded_state['optimizer_state_dict'], \"Optimizer state_dict mismatch\"\n",
    "\n",
    "# 创建一个实例并运行测试\n",
    "test = TestLoadCheckpoint()\n",
    "test.setUp()\n",
    "test.test_load_checkpoint()\n",
    "test.tearDown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quxianshibie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
